{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.metrics import MeanSquaredError, Accuracy\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data(filename, date_col, start_date, seperator, fill, lim):\n",
    "    df_read = pd.read_csv(filename, sep=seperator)\n",
    "    \n",
    "    # Convert the date column to datetime and set it as the index\n",
    "    df_read[date_col] = pd.to_datetime(df_read[date_col])\n",
    "    df_read.set_index(date_col, inplace=True)\n",
    "    \n",
    "    # Data cleaning: replace commas with periods and convert to float\n",
    "    for column in df_read.columns:\n",
    "        if df_read[column].dtype == 'object':\n",
    "            df_read[column] = df_read[column].str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Start dataset from start date\n",
    "    df_filtered = df_read[start_date:]\n",
    "\n",
    "    # Fill missing values\n",
    "    df_filtered.fillna(fill, limit=lim)\n",
    "    \n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_features(df, windows):\n",
    "\n",
    "    feature_cols = [col for col in df.columns if 'macro' not in col.lower()]\n",
    "    macro_cols = [col for col in df.columns if 'MACRO' in col.upper()]\n",
    "    # Initialize a dictionary to hold all the new feature data\n",
    "    features_dict = {}\n",
    "\n",
    "\n",
    "    # Perform rolling calculations for each window size\n",
    "    for w in windows:\n",
    "        for col in feature_cols:\n",
    "            # Create unique feature names for each statistic and window size\n",
    "            features_dict[f'{col}_VaR_{w}'] = df[col].rolling(window=w, min_periods=int(w//2)).quantile(0.05)\n",
    "            features_dict[f'{col}_momentum_{w}'] = df[col].rolling(window=w, min_periods=int(w//2)).sum()\n",
    "            features_dict[f'{col}_avgreturn_{w}'] = df[col].rolling(window=w, min_periods=int(w//2)).mean()\n",
    "            features_dict[f'{col}_skew_{w}'] = df[col].rolling(window=w, min_periods=int(w//2)).skew()\n",
    "            features_dict[f'{col}_volatility_{w}'] = df[col].rolling(window=w, min_periods=int(w//2)).std()\n",
    "\n",
    "    # Convert the dictionary of Series to a DataFrame\n",
    "    features_df = pd.DataFrame(features_dict, index=df.index)\n",
    "\n",
    "    # Concatenate 'MACRO' columns to the features DataFrame\n",
    "    if macro_cols:\n",
    "        macro_df = df[macro_cols]\n",
    "        features_df = pd.concat([features_df, macro_df], axis=1)\n",
    "\n",
    "    return features_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_and_pivot_df(df, date_col):\n",
    "    # Reset the index to make the date a regular column\n",
    "    df_reset = df.reset_index()\n",
    "    \n",
    "    # Melt the DataFrame to long format\n",
    "    long_df = df_reset.melt(id_vars=date_col, var_name='metric', value_name='value')\n",
    "    \n",
    "    # Split the 'metric' column to extract components\n",
    "    split_metrics = long_df['metric'].str.split('_', expand=True)\n",
    "    \n",
    "    # Identify 'MACRO' rows\n",
    "    macro_mask = split_metrics[0] == 'MACRO'\n",
    "    \n",
    "    # For non-'MACRO' metrics, define 'asset' and 'metric_type'\n",
    "    long_df['asset'] = split_metrics[0] + '_' + split_metrics[1]\n",
    "    long_df['metric_type'] = split_metrics[2] + '_' + split_metrics[3]\n",
    "\n",
    "    # Reset index on the left-hand side DataFrame slice to ensure alignment\n",
    "    lhs = long_df.loc[macro_mask, 'metric_type'].reset_index(drop=True)\n",
    "\n",
    "    # Reset index on the right-hand side Series to ensure alignment\n",
    "    rhs = (split_metrics.loc[macro_mask, 0] + '_' + split_metrics.loc[macro_mask, 1]).reset_index(drop=True)\n",
    "\n",
    "    # Assign the values after ensuring both sides have the same length\n",
    "    lhs = rhs\n",
    "\n",
    "    # Assign the modified Series back to the original DataFrame (if needed)\n",
    "    long_df.loc[macro_mask, 'metric_type'] = lhs.values\n",
    "    \n",
    "    # For 'MACRO' metrics, adjust 'metric_type' and 'asset'\n",
    "    long_df.loc[macro_mask, 'metric_type'] = split_metrics.loc[macro_mask, 0] + '_' + split_metrics.loc[macro_mask, 1]\n",
    "    long_df.loc[macro_mask, 'asset'] = 'MACRO'\n",
    "    \n",
    "    # Remove 'MACRO' placeholder rows\n",
    "    long_df = long_df[long_df['asset'] != 'MACRO']\n",
    "    \n",
    "    # Pivot the DataFrame back to wide format\n",
    "    final_df = long_df.pivot_table(index=[date_col, 'asset'], columns='metric_type', values='value').reset_index()\n",
    "    \n",
    "    # Handle 'MACRO' metrics separately\n",
    "    macro_df = df.filter(regex='^MACRO').copy()\n",
    "    macro_df[date_col] = df_reset[date_col]\n",
    "    \n",
    "    # Merge 'MACRO' metrics back into the final DataFrame\n",
    "    if date_col in macro_df.columns:\n",
    "        macro_df = macro_df.drop(columns=date_col)\n",
    "    \n",
    "    macro_df = macro_df.reset_index()\n",
    "    final_df = pd.merge(final_df, macro_df, on=date_col, how='left')\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_y_col(df, date_col, target_days, return_col, volatility_col):\n",
    "    # Shift the specified return and volatility columns by the target number of days\n",
    "    df[f'{return_col}_shifted'] = df.groupby('asset')[return_col].shift(-target_days)\n",
    "    df[f'{volatility_col}_shifted'] = df.groupby('asset')[volatility_col].shift(-target_days)\n",
    "    \n",
    "    # Drop rows with NaN values that result from the shift operation\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Calculate the Sharpe ratio by dividing the shifted return by the shifted volatility\n",
    "    df['sharpe_ratio'] = df[f'{return_col}_shifted'] / df[f'{volatility_col}_shifted']\n",
    "    \n",
    "    # Calculate the mean Sharpe ratio for each date and merge it back into the DataFrame\n",
    "    sharpe_ratio_mean = df.groupby(date_col)['sharpe_ratio'].mean().rename('sharpe_ratio_mean')\n",
    "    df = df.merge(sharpe_ratio_mean, on=date_col)\n",
    "    \n",
    "    # Create a new binary column 'Y', indicating whether the Sharpe ratio is above the mean for its date\n",
    "    df['Y'] = np.where(df['sharpe_ratio'] > df['sharpe_ratio_mean'], 1, 0)\n",
    "\n",
    "    df = df.drop(columns=['sharpe_ratio', 'sharpe_ratio_mean', f'{return_col}_shifted', f'{volatility_col}_shifted'])\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_training_dataset(df, date_col, target_col='Y', shuffle=False, train_split=0.25, eval_split=0.25):\n",
    "    # Encode categorical variables if necessary (assuming 'asset' is the only categorical variable)\n",
    "    df_encoded = pd.get_dummies(df, columns=['asset'], drop_first=True)\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns=['Y', 'asset'])\n",
    "\n",
    "    y = df['Y']\n",
    "\n",
    "    # Convert date column to datetime if not already done\n",
    "    X[date_col] = pd.to_datetime(X[date_col])\n",
    "\n",
    "    if shuffle:\n",
    "        # Split the data randomly\n",
    "        train_size = 1 - train_split\n",
    "        X_temp, X_train, y_temp, y_train = train_test_split(X, y, train_size=train_size)\n",
    "        X_eval, X_test, y_eval, y_test = train_test_split(X_temp, y_temp, test_size=(2/3))\n",
    "    else:\n",
    "        # Split the data sequentially\n",
    "        train_end_idx = int(len(X) * train_split)\n",
    "        eval_end_idx = train_end_idx + int(len(X) * eval_split)\n",
    "\n",
    "        X_train = X.iloc[:train_end_idx]\n",
    "        y_train = y.iloc[:train_end_idx]\n",
    "        X_eval = X.iloc[train_end_idx:eval_end_idx]\n",
    "        y_eval = y.iloc[train_end_idx:eval_end_idx]\n",
    "        X_test = X.iloc[eval_end_idx:]\n",
    "        y_test = y.iloc[eval_end_idx:]\n",
    "\n",
    "    # Drop the date column\n",
    "    X_train = X_train.drop(date_col, axis=1)\n",
    "    X_eval = X_eval.drop(date_col, axis=1)\n",
    "    X_test = X_test.drop(date_col, axis=1)\n",
    "\n",
    "    # Combine training and evaluation sets for the final model training\n",
    "    X_train_eval = pd.concat([X_train, X_eval])\n",
    "    y_train_eval = pd.concat([y_train, y_eval])\n",
    "\n",
    "\n",
    "    return X_train, X_eval, X_test, y_train, y_eval, y_test, X_train_eval, y_train_eval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_and_train_ridge(X_train, y_train, X_train_eval, y_train_eval, param_grid, scoring='accuracy', cv=5):\n",
    "\n",
    "    model = RidgeClassifier()\n",
    "\n",
    "    # Initialize GridSearchCV with the provided model and parameter grid\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring=scoring, cv=cv)\n",
    "    \n",
    "    # Fit GridSearchCV on the training set\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best parameters and the accuracy on the evaluation set\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    print(\"Best accuracy on evaluation set:\", grid_search.best_score_)\n",
    "    \n",
    "    # Retrain the model with the best parameters on the combined training and evaluation sets\n",
    "    model_best = model.__class__(**grid_search.best_params_)\n",
    "    model_best.fit(X_train_eval, y_train_eval)\n",
    "\n",
    "    return model_best, grid_search\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model_performance(y_true, y_pred):\n",
    " \n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Print the performance metrics\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred)}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "\n",
    "    return conf_matrix, precision, recall, f1, mse, rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_and_train_xgb(X_train, y_train, X_eval, y_eval, param_grid, scoring='accuracy', cv=5, n_jobs=-1, early_stopping_rounds=10):\n",
    "\n",
    "    # Initialize the XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, scoring=scoring, cv=cv, n_jobs=n_jobs)\n",
    "    grid_search.fit(X_train, y_train, eval_set=[(X_eval, y_eval)], early_stopping_rounds=early_stopping_rounds, verbose=False)\n",
    "\n",
    "    # Extract best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    # Retrain the model with the best parameters on the combined training and evaluation set\n",
    "    xgb_best = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_best.fit(pd.concat([X_train, X_eval]), pd.concat([y_train, y_eval]))\n",
    "\n",
    "    return xgb_best, best_params\n",
    "\n",
    "# Define param_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize_and_train_lgb(X_train, y_train, X_eval, y_eval, param_grid, scoring='accuracy', cv=5, n_jobs=-1):\n",
    "\n",
    "    # Initialize the LightGBM model\n",
    "    lgb_model = lgb.LGBMClassifier()\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(lgb_model, param_grid, scoring=scoring, cv=cv, n_jobs=n_jobs)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "    # Retrain the model with the best parameters on the combined training and evaluation set\n",
    "    lgb_best = lgb.LGBMClassifier(**best_params)\n",
    "    lgb_best.fit(pd.concat([X_train, X_eval]), pd.concat([y_train, y_eval]), eval_set=[(X_eval, y_eval)])\n",
    "\n",
    "    return lgb_best, best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_evaluate_NN(X_train_eval, y_train_eval, X_eval, y_eval, X_test, y_test, epochs=50, batch_size=32):\n",
    "\n",
    "    # Initialize the scaler and scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_eval_scaled = scaler.fit_transform(X_train_eval)\n",
    "    X_eval_scaled = scaler.transform(X_eval)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define the model architecture\n",
    "    #undersök relu\n",
    "    model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_eval_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    # Add two more relu activated layers\n",
    "    Dense(16, activation='relu'), \n",
    "    Dense(8, activation='relu'),   \n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_eval_scaled, y_train_eval,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_eval_scaled, y_eval)\n",
    "    )\n",
    "    return model, history, X_test_scaled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\appl\\ipykernel_8428\\1426980292.py:2: DtypeWarning: Columns (15,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_read = pd.read_csv(filename, sep=seperator)\n"
     ]
    }
   ],
   "source": [
    "filename = 'all_data_anonymized.csv'\n",
    "dateCol = 'todate'\n",
    "start_date = '1980-01-01'\n",
    "seperator = ';'\n",
    "fill = 0\n",
    "lim = 5\n",
    "df = load_and_preprocess_data(filename, dateCol, start_date, seperator, fill, lim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [5, 10, 20, 40, 60, 100, 180, 240, 360, 480]\n",
    "window_m = [10, 30, 60, 100, 180]\n",
    "assets = df.columns\n",
    "\n",
    "df = add_features(df, window_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_and_pivot_df(df, dateCol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\appl\\ipykernel_8428\\1208930837.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sharpe_ratio'] = df[f'{return_col}_shifted'] / df[f'{volatility_col}_shifted']\n"
     ]
    }
   ],
   "source": [
    "target_days = 9\n",
    "return_column_shift = 'avgreturn_10'\n",
    "volatility_column_shift = 'volatility_10'\n",
    "df = add_y_col(df, dateCol, target_days, return_column_shift, volatility_column_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_eval, X_test, y_train, y_eval, y_test, X_train_eval, y_train_eval = prepare_training_dataset(df, dateCol, target_col='Y', shuffle=False, train_split=0.25, eval_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 10.0}\n",
      "Best accuracy on evaluation set: 0.5245591820194883\n"
     ]
    }
   ],
   "source": [
    "param_grid_alpha = {'alpha': [0.1, 1.0, 10.0]}\n",
    "ridge_best, grid_search = optimize_and_train_ridge(X_train, y_train, X_train_eval, y_train_eval, param_grid_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\appl\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_best, best_params = optimize_and_train_xgb(X_train, y_train, X_eval, y_eval, param_grid_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 47567, number of negative: 49810\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 97377, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.488483 -> initscore=-0.046077\n",
      "[LightGBM] [Info] Start training from score -0.046077\n",
      "Best hyperparameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'num_leaves': 31}\n",
      "[LightGBM] [Info] Number of positive: 95466, number of negative: 99288\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 194754, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.490188 -> initscore=-0.039255\n",
      "[LightGBM] [Info] Start training from score -0.039255\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_lgb = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [31, 63, 127, 255]\n",
    "}\n",
    "\n",
    "# Call the function with your datasets and hyperparameter grid\n",
    "lgb_best, best_params = optimize_and_train_lgb(X_train, y_train, X_eval, y_eval, param_grid_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\appl\\Anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.5453 - loss: 0.2471 - val_accuracy: 0.5527 - val_loss: 0.2446\n",
      "Epoch 2/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5543 - loss: 0.2446 - val_accuracy: 0.5589 - val_loss: 0.2430\n",
      "Epoch 3/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5681 - loss: 0.2421 - val_accuracy: 0.5663 - val_loss: 0.2412\n",
      "Epoch 4/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5734 - loss: 0.2407 - val_accuracy: 0.5775 - val_loss: 0.2386\n",
      "Epoch 5/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5813 - loss: 0.2388 - val_accuracy: 0.5843 - val_loss: 0.2379\n",
      "Epoch 6/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5856 - loss: 0.2378 - val_accuracy: 0.5914 - val_loss: 0.2357\n",
      "Epoch 7/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5880 - loss: 0.2368 - val_accuracy: 0.5825 - val_loss: 0.2371\n",
      "Epoch 8/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5914 - loss: 0.2354 - val_accuracy: 0.5948 - val_loss: 0.2335\n",
      "Epoch 9/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5980 - loss: 0.2342 - val_accuracy: 0.5996 - val_loss: 0.2327\n",
      "Epoch 10/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6020 - loss: 0.2330 - val_accuracy: 0.6013 - val_loss: 0.2319\n",
      "Epoch 11/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6049 - loss: 0.2320 - val_accuracy: 0.6047 - val_loss: 0.2315\n",
      "Epoch 12/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6046 - loss: 0.2316 - val_accuracy: 0.6099 - val_loss: 0.2298\n",
      "Epoch 13/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6060 - loss: 0.2307 - val_accuracy: 0.6091 - val_loss: 0.2301\n",
      "Epoch 14/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6109 - loss: 0.2300 - val_accuracy: 0.6132 - val_loss: 0.2280\n",
      "Epoch 15/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6133 - loss: 0.2290 - val_accuracy: 0.6146 - val_loss: 0.2286\n",
      "Epoch 16/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6139 - loss: 0.2288 - val_accuracy: 0.6130 - val_loss: 0.2283\n",
      "Epoch 17/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6135 - loss: 0.2281 - val_accuracy: 0.6224 - val_loss: 0.2259\n",
      "Epoch 18/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6168 - loss: 0.2274 - val_accuracy: 0.6190 - val_loss: 0.2267\n",
      "Epoch 19/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6194 - loss: 0.2269 - val_accuracy: 0.6214 - val_loss: 0.2257\n",
      "Epoch 20/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6197 - loss: 0.2265 - val_accuracy: 0.6248 - val_loss: 0.2250\n",
      "Epoch 21/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6212 - loss: 0.2260 - val_accuracy: 0.6202 - val_loss: 0.2255\n",
      "Epoch 22/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6225 - loss: 0.2255 - val_accuracy: 0.6274 - val_loss: 0.2243\n",
      "Epoch 23/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6240 - loss: 0.2253 - val_accuracy: 0.6305 - val_loss: 0.2231\n",
      "Epoch 24/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6260 - loss: 0.2246 - val_accuracy: 0.6320 - val_loss: 0.2226\n",
      "Epoch 25/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6302 - loss: 0.2237 - val_accuracy: 0.6246 - val_loss: 0.2250\n",
      "Epoch 26/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.6284 - loss: 0.2236 - val_accuracy: 0.6324 - val_loss: 0.2226\n",
      "Epoch 27/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6311 - loss: 0.2231 - val_accuracy: 0.6347 - val_loss: 0.2215\n",
      "Epoch 28/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6311 - loss: 0.2230 - val_accuracy: 0.6318 - val_loss: 0.2227\n",
      "Epoch 29/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6312 - loss: 0.2225 - val_accuracy: 0.6347 - val_loss: 0.2218\n",
      "Epoch 30/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6335 - loss: 0.2222 - val_accuracy: 0.6380 - val_loss: 0.2207\n",
      "Epoch 31/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6328 - loss: 0.2222 - val_accuracy: 0.6364 - val_loss: 0.2217\n",
      "Epoch 32/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6340 - loss: 0.2221 - val_accuracy: 0.6390 - val_loss: 0.2206\n",
      "Epoch 33/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6329 - loss: 0.2222 - val_accuracy: 0.6405 - val_loss: 0.2197\n",
      "Epoch 34/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6359 - loss: 0.2213 - val_accuracy: 0.6390 - val_loss: 0.2200\n",
      "Epoch 35/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6385 - loss: 0.2206 - val_accuracy: 0.6409 - val_loss: 0.2200\n",
      "Epoch 36/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6364 - loss: 0.2212 - val_accuracy: 0.6375 - val_loss: 0.2207\n",
      "Epoch 37/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6390 - loss: 0.2203 - val_accuracy: 0.6411 - val_loss: 0.2195\n",
      "Epoch 38/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6385 - loss: 0.2202 - val_accuracy: 0.6431 - val_loss: 0.2190\n",
      "Epoch 39/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.6398 - loss: 0.2201 - val_accuracy: 0.6449 - val_loss: 0.2183\n",
      "Epoch 40/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6404 - loss: 0.2194 - val_accuracy: 0.6449 - val_loss: 0.2182\n",
      "Epoch 41/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6425 - loss: 0.2193 - val_accuracy: 0.6443 - val_loss: 0.2186\n",
      "Epoch 42/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6409 - loss: 0.2194 - val_accuracy: 0.6404 - val_loss: 0.2189\n",
      "Epoch 43/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6416 - loss: 0.2195 - val_accuracy: 0.6449 - val_loss: 0.2179\n",
      "Epoch 44/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6417 - loss: 0.2194 - val_accuracy: 0.6463 - val_loss: 0.2176\n",
      "Epoch 45/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6425 - loss: 0.2188 - val_accuracy: 0.6483 - val_loss: 0.2172\n",
      "Epoch 46/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6456 - loss: 0.2181 - val_accuracy: 0.6491 - val_loss: 0.2169\n",
      "Epoch 47/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6442 - loss: 0.2186 - val_accuracy: 0.6485 - val_loss: 0.2168\n",
      "Epoch 48/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6440 - loss: 0.2184 - val_accuracy: 0.6449 - val_loss: 0.2181\n",
      "Epoch 49/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.6450 - loss: 0.2179 - val_accuracy: 0.6501 - val_loss: 0.2165\n",
      "Epoch 50/50\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.6447 - loss: 0.2190 - val_accuracy: 0.6507 - val_loss: 0.2159\n"
     ]
    }
   ],
   "source": [
    "NN_model, history, X_test_scaled = train_and_evaluate_NN(X_train_eval, y_train_eval, X_eval, y_eval, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Classifier:\n",
      "Accuracy: 0.5247308184044732\n",
      "Confusion Matrix:\n",
      "[[58663 40583]\n",
      " [51979 43532]]\n",
      "Precision: 0.5175295726089283\n",
      "Recall: 0.4557799625174064\n",
      "F1 Score: 0.4846959794239141\n",
      "MSE: 0.4752691815955267\n",
      "RMSE: 0.6893976947999803\n",
      "\n",
      "\n",
      "XGBoost Classifier:\n",
      "Accuracy: 0.5290079432318222\n",
      "Confusion Matrix:\n",
      "[[56632 42614]\n",
      " [49115 46396]]\n",
      "Precision: 0.5212448039546118\n",
      "Recall: 0.48576603741977364\n",
      "F1 Score: 0.5028804309536584\n",
      "MSE: 0.4709920567681778\n",
      "RMSE: 0.6862886104024879\n",
      "\n",
      "\n",
      "LightGBM Classifier:\n",
      "Accuracy: 0.5280734453703846\n",
      "Confusion Matrix:\n",
      "[[57571 41675]\n",
      " [50236 45275]]\n",
      "Precision: 0.5207015526164462\n",
      "Recall: 0.4740291694150412\n",
      "F1 Score: 0.4962704358739676\n",
      "MSE: 0.47192655462961536\n",
      "RMSE: 0.6869691074783606\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store predictions in a dictionary\n",
    "predictions = {\n",
    "    \"Ridge Classifier\": ridge_best.predict(X_test),\n",
    "    \"XGBoost Classifier\": xgb_best.predict(X_test),\n",
    "    \"LightGBM Classifier\": lgb_best.predict(X_test),\n",
    "}   \n",
    "\n",
    "# Iterate through the dictionary and evaluate each model\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(model_name + \":\")\n",
    "    evaluate_model_performance(y_test, y_pred)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 854us/step\n",
      "Confusion Matrix:\n",
      "[[50965 48281]\n",
      " [46212 49299]]\n",
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 965us/step - accuracy: 0.5175 - loss: 0.2720\n",
      "Test Loss: 0.2734670341014862, Test Accuracy: 0.5148159265518188\n",
      "Precision: 0.5150202857183219\n",
      "Recall: 0.5148158987866932\n",
      "F1-Score: 0.5148599936171839\n",
      "MSE: 0.48518410121330685\n",
      "RMSE: 0.696551578286423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "y_pred_nn = NN_model.predict(X_test_scaled)\n",
    "test_loss, test_accuracy = NN_model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "y_pred_labels = (y_pred_nn > 0.5).astype(int)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred_labels, average='weighted'  # Use 'weighted' for multiclass classification\n",
    ")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "mse = mean_squared_error(y_test, y_pred_labels)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6087/6087\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 793us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = NN_model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43421495]\n",
      " [0.46601734]\n",
      " [0.5518875 ]\n",
      " ...\n",
      " [0.4870377 ]\n",
      " [0.427313  ]\n",
      " [0.62956953]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.argmax(y_pred, axis=1)\n",
    "# print amount of 1s and 0s\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_lgb = lgb_best.predict(X_test)\n",
    "y_pred_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

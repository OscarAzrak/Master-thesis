{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from func import *\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oscar.azrak\\Desktop\\Master-thesis\\Master-thesis\\Files\\func.py:21: DtypeWarning: Columns (15,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_read = pd.read_csv(filename, sep=seperator)\n"
     ]
    }
   ],
   "source": [
    "filename = 'all_data_anonymized.csv'\n",
    "date_col = 'todate'\n",
    "start_date = '1980-01-01'\n",
    "seperator = ';'\n",
    "fill = 0\n",
    "lim = 5\n",
    "df_read = load_and_preprocess_data(filename, date_col, start_date, seperator, fill, lim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [5, 10, 20, 40, 60, 100, 180, 240, 360, 480]\n",
    "window_m = [10, 30, 60, 100, 180]\n",
    "assets = df_read.columns\n",
    "df_feat = add_features(df_read, window_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_days = 9\n",
    "\n",
    "return_column_shift = 'avgreturn'\n",
    "volatility_column_shift = 'volatility'\n",
    "df = add_y_col(df_feat, df_read, date_col, target_days, return_column_shift, volatility_column_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, X_test, y_train, y_eval, y_test, X_train_eval, y_train_eval = prepare_training_dataset(df, date_col, shuffle=False, train_split=0.25, eval_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 10.0}\n",
      "Best accuracy on evaluation set: 0.5086063469241039\n"
     ]
    }
   ],
   "source": [
    "param_grid_alpha = {'alpha': [0.1, 1.0, 10.0]}\n",
    "ridge_best, grid_search = optimize_and_train_ridge(X_train, y_train, X_train_eval, y_train_eval, param_grid_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar.azrak\\AppData\\Roaming\\Python\\Python310\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_best, best_params = optimize_and_train_xgb(X_train, y_train, X_eval, y_eval, param_grid_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47869, number of negative: 49501\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 97370, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491620 -> initscore=-0.033525\n",
      "[LightGBM] [Info] Start training from score -0.033525\n",
      "Best hyperparameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 200, 'num_leaves': 31}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 95752, number of negative: 98988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 194740, number of used features: 34\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491691 -> initscore=-0.033237\n",
      "[LightGBM] [Info] Start training from score -0.033237\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_lgb = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [31, 63, 127, 255]\n",
    "}\n",
    "\n",
    "# Call the function with your datasets and hyperparameter grid\n",
    "lgb_best, best_params = optimize_and_train_lgb(X_train, y_train, X_eval, y_eval, param_grid_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar.azrak\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5198 - loss: 0.2498 - val_accuracy: 0.5438 - val_loss: 0.2475\n",
      "Epoch 2/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5355 - loss: 0.2482 - val_accuracy: 0.5452 - val_loss: 0.2471\n",
      "Epoch 3/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5385 - loss: 0.2476 - val_accuracy: 0.5461 - val_loss: 0.2469\n",
      "Epoch 4/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5441 - loss: 0.2467 - val_accuracy: 0.5521 - val_loss: 0.2459\n",
      "Epoch 5/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.5501 - loss: 0.2461 - val_accuracy: 0.5579 - val_loss: 0.2447\n",
      "Epoch 6/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5551 - loss: 0.2452 - val_accuracy: 0.5650 - val_loss: 0.2438\n",
      "Epoch 7/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5632 - loss: 0.2439 - val_accuracy: 0.5638 - val_loss: 0.2435\n",
      "Epoch 8/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.5639 - loss: 0.2435 - val_accuracy: 0.5706 - val_loss: 0.2423\n",
      "Epoch 9/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5659 - loss: 0.2428 - val_accuracy: 0.5719 - val_loss: 0.2417\n",
      "Epoch 10/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.5683 - loss: 0.2418 - val_accuracy: 0.5752 - val_loss: 0.2411\n",
      "Epoch 11/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.5752 - loss: 0.2409 - val_accuracy: 0.5751 - val_loss: 0.2405\n",
      "Epoch 12/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5771 - loss: 0.2401 - val_accuracy: 0.5820 - val_loss: 0.2391\n",
      "Epoch 13/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5803 - loss: 0.2392 - val_accuracy: 0.5844 - val_loss: 0.2385\n",
      "Epoch 14/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.5835 - loss: 0.2385 - val_accuracy: 0.5824 - val_loss: 0.2386\n",
      "Epoch 15/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.5857 - loss: 0.2379 - val_accuracy: 0.5894 - val_loss: 0.2370\n",
      "Epoch 16/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5879 - loss: 0.2371 - val_accuracy: 0.5878 - val_loss: 0.2376\n",
      "Epoch 17/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.5894 - loss: 0.2365 - val_accuracy: 0.5911 - val_loss: 0.2368\n",
      "Epoch 18/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5893 - loss: 0.2366 - val_accuracy: 0.5901 - val_loss: 0.2368\n",
      "Epoch 19/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5911 - loss: 0.2363 - val_accuracy: 0.5930 - val_loss: 0.2363\n",
      "Epoch 20/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5943 - loss: 0.2353 - val_accuracy: 0.5941 - val_loss: 0.2358\n",
      "Epoch 21/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.5953 - loss: 0.2349 - val_accuracy: 0.5963 - val_loss: 0.2350\n",
      "Epoch 22/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6002 - loss: 0.2342 - val_accuracy: 0.5948 - val_loss: 0.2357\n",
      "Epoch 23/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.5957 - loss: 0.2345 - val_accuracy: 0.5986 - val_loss: 0.2343\n",
      "Epoch 24/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5978 - loss: 0.2341 - val_accuracy: 0.6019 - val_loss: 0.2338\n",
      "Epoch 25/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6000 - loss: 0.2337 - val_accuracy: 0.6017 - val_loss: 0.2337\n",
      "Epoch 26/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6019 - loss: 0.2333 - val_accuracy: 0.6028 - val_loss: 0.2334\n",
      "Epoch 27/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6032 - loss: 0.2326 - val_accuracy: 0.6033 - val_loss: 0.2331\n",
      "Epoch 28/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6067 - loss: 0.2313 - val_accuracy: 0.6059 - val_loss: 0.2322\n",
      "Epoch 29/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6040 - loss: 0.2322 - val_accuracy: 0.6113 - val_loss: 0.2314\n",
      "Epoch 30/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6062 - loss: 0.2318 - val_accuracy: 0.6028 - val_loss: 0.2328\n",
      "Epoch 31/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6075 - loss: 0.2312 - val_accuracy: 0.6101 - val_loss: 0.2312\n",
      "Epoch 32/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6083 - loss: 0.2314 - val_accuracy: 0.6084 - val_loss: 0.2312\n",
      "Epoch 33/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6089 - loss: 0.2313 - val_accuracy: 0.6112 - val_loss: 0.2307\n",
      "Epoch 34/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6131 - loss: 0.2302 - val_accuracy: 0.6091 - val_loss: 0.2313\n",
      "Epoch 35/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6126 - loss: 0.2301 - val_accuracy: 0.6150 - val_loss: 0.2297\n",
      "Epoch 36/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6130 - loss: 0.2299 - val_accuracy: 0.6115 - val_loss: 0.2303\n",
      "Epoch 37/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6121 - loss: 0.2304 - val_accuracy: 0.6129 - val_loss: 0.2299\n",
      "Epoch 38/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6104 - loss: 0.2301 - val_accuracy: 0.6155 - val_loss: 0.2297\n",
      "Epoch 39/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6146 - loss: 0.2295 - val_accuracy: 0.6134 - val_loss: 0.2298\n",
      "Epoch 40/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6156 - loss: 0.2292 - val_accuracy: 0.6203 - val_loss: 0.2286\n",
      "Epoch 41/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6156 - loss: 0.2289 - val_accuracy: 0.6145 - val_loss: 0.2297\n",
      "Epoch 42/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6169 - loss: 0.2288 - val_accuracy: 0.6195 - val_loss: 0.2281\n",
      "Epoch 43/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6178 - loss: 0.2287 - val_accuracy: 0.6164 - val_loss: 0.2291\n",
      "Epoch 44/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6189 - loss: 0.2278 - val_accuracy: 0.6180 - val_loss: 0.2287\n",
      "Epoch 45/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6187 - loss: 0.2283 - val_accuracy: 0.6193 - val_loss: 0.2283\n",
      "Epoch 46/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6202 - loss: 0.2275 - val_accuracy: 0.6202 - val_loss: 0.2285\n",
      "Epoch 47/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6180 - loss: 0.2283 - val_accuracy: 0.6223 - val_loss: 0.2270\n",
      "Epoch 48/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - accuracy: 0.6199 - loss: 0.2278 - val_accuracy: 0.6236 - val_loss: 0.2269\n",
      "Epoch 49/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6206 - loss: 0.2275 - val_accuracy: 0.6186 - val_loss: 0.2280\n",
      "Epoch 50/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.6182 - loss: 0.2278 - val_accuracy: 0.6206 - val_loss: 0.2275\n"
     ]
    }
   ],
   "source": [
    "NN_model, history, X_test_scaled = train_and_evaluate_NN(X_train_eval, y_train_eval, X_eval, y_eval, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Ridge Classifier:\n",
      "Accuracy: 0.5281781630148452\n",
      "Confusion Matrix:\n",
      "[[70783 28420]\n",
      " [63464 32076]]\n",
      "Precision: 0.5302168738428987\n",
      "Recall: 0.33573372409462005\n",
      "F1 Score: 0.411135891717296\n",
      "MSE: 0.4718218369851548\n",
      "RMSE: 0.6868928861075465\n",
      "\n",
      "\n",
      "XGBoost Classifier:\n",
      "Accuracy: 0.5266325362143954\n",
      "Confusion Matrix:\n",
      "[[58120 41083]\n",
      " [51102 44438]]\n",
      "Precision: 0.5196150653055974\n",
      "Recall: 0.46512455516014234\n",
      "F1 Score: 0.490862195613633\n",
      "MSE: 0.4733674637856046\n",
      "RMSE: 0.6880170519584559\n",
      "\n",
      "\n",
      "LightGBM Classifier:\n",
      "Accuracy: 0.5251279891960173\n",
      "Confusion Matrix:\n",
      "[[59317 39886]\n",
      " [52592 42948]]\n",
      "Precision: 0.5184827486297897\n",
      "Recall: 0.4495289930918987\n",
      "F1 Score: 0.48155000168185946\n",
      "MSE: 0.47487201080398267\n",
      "RMSE: 0.6891095782268468\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store predictions in a dictionary\n",
    "predictions = {\n",
    "    \"Ridge Classifier\": ridge_best.predict(X_test),\n",
    "    \"XGBoost Classifier\": xgb_best.predict(X_test),\n",
    "    \"LightGBM Classifier\": lgb_best.predict(X_test),\n",
    "}   \n",
    "\n",
    "# Iterate through the dictionary and evaluate each model\n",
    "for model_name, y_pred in predictions.items():\n",
    "    print(model_name + \":\")\n",
    "    evaluate_model_performance(y_test, y_pred)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features_ is 34 and input n_features is 38",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# lgb score predict\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred_prob_lgb \u001b[38;5;241m=\u001b[39m \u001b[43mlgb_best\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m]  \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLightGBM Classifier Prediction Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred_prob_lgb)\n\u001b[0;32m      5\u001b[0m y_pred_prob_xgb \u001b[38;5;241m=\u001b[39m xgb_best\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]  \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\sklearn.py:1253\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1243\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m   1251\u001b[0m ):\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1253\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m   1254\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   1255\u001b[0m         raw_score\u001b[38;5;241m=\u001b[39mraw_score,\n\u001b[0;32m   1256\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[0;32m   1257\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[0;32m   1258\u001b[0m         pred_leaf\u001b[38;5;241m=\u001b[39mpred_leaf,\n\u001b[0;32m   1259\u001b[0m         pred_contrib\u001b[38;5;241m=\u001b[39mpred_contrib,\n\u001b[0;32m   1260\u001b[0m         validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[0;32m   1261\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1262\u001b[0m     )\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n\u001b[0;32m   1264\u001b[0m         _log_warning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot compute class probabilities or labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1265\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1266\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning raw scores instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\lightgbm\\sklearn.py:937\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m!=\u001b[39m n_features:\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features of the model must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch the input. Model n_features_ is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput n_features is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;66;03m# retrive original params that possibly can be used in both training and prediction\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;66;03m# and then overwrite them (considering aliases) with params that were passed directly in prediction\u001b[39;00m\n\u001b[0;32m    942\u001b[0m predict_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features_ is 34 and input n_features is 38"
     ]
    }
   ],
   "source": [
    "# lgb score predict\n",
    "y_pred_prob_lgb = lgb_best.predict_proba(X_test)[:, 1]  \n",
    "print(\"LightGBM Classifier Prediction Scores:\", y_pred_prob_lgb)\n",
    "\n",
    "y_pred_prob_xgb = xgb_best.predict_proba(X_test)[:, 1]  \n",
    "print(\"XGBoost Classifier Prediction Scores:\", y_pred_prob_xgb)\n",
    "\n",
    "y_scores = ridge_best.decision_function(X_test)\n",
    "y_pred_prob_ridge = sigmoid(y_scores)  # Convert scores to probabilities using sigmoid\n",
    "print(\"Ridge Classifier Prediction Scores:\", y_pred_prob_ridge)\n",
    "\n",
    "y_pred_prob_NN = NN_model.predict(X_test_scaled)  \n",
    "print(\"NN Classifier Prediction Scores:\", y_pred_prob_NN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['lgb'] = y_pred_prob_lgb\n",
    "X_test['xgb'] = y_pred_prob_xgb\n",
    "X_test['ridge'] = y_pred_prob_ridge\n",
    "X_test['NN'] = y_pred_prob_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['VaR_10', 'VaR_100', 'VaR_180', 'VaR_30', 'VaR_60', 'avgreturn_10',\n",
      "       'avgreturn_100', 'avgreturn_180', 'avgreturn_30', 'avgreturn_60',\n",
      "       'momentum_10', 'momentum_100', 'momentum_180', 'momentum_30',\n",
      "       'momentum_60', 'skew_10', 'skew_100', 'skew_180', 'skew_30', 'skew_60',\n",
      "       'volatility_10', 'volatility_100', 'volatility_180', 'volatility_30',\n",
      "       'volatility_60', 'MACRO_8', 'MACRO_0', 'MACRO_1', 'MACRO_2', 'MACRO_3',\n",
      "       'MACRO_4', 'MACRO_5', 'MACRO_6', 'MACRO_7', 'lgb', 'xgb', 'ridge',\n",
      "       'NN'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 best and worst assets for each class and only keep asset column\n",
    "\n",
    "top_10_best_assets_lgb = X_test.sort_values(by='lgb', ascending=False).head(10)\n",
    "top_10_worst_assets_lgb = X_test.sort_values(by='lgb', ascending=True).head(10)\n",
    "\n",
    "top_10_best_assets_xgb = X_test.sort_values(by='xgb', ascending=False).head(10)\n",
    "top_10_worst_assets_xgb = X_test.sort_values(by='xgb', ascending=True).head(10)\n",
    "\n",
    "top_10_best_assets_ridge = X_test.sort_values(by='ridge', ascending=False).head(10)\n",
    "top_10_worst_assets_ridge = X_test.sort_values(by='ridge', ascending=True).head(10)\n",
    "\n",
    "top_10_best_assets_NN = X_test.sort_values(by='NN', ascending=False).head(10)\n",
    "top_10_worst_assets_NN = X_test.sort_values(by='NN', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 best assets for LightGBM Classifier:\n",
      "          VaR_10   VaR_100   VaR_180    VaR_30    VaR_60  avgreturn_10  \\\n",
      "425846 -0.002505 -0.003825 -0.004294 -0.003351 -0.003825      0.002842   \n",
      "425672 -0.002505 -0.003924 -0.004709 -0.003825 -0.003825      0.003524   \n",
      "425498 -0.001599 -0.003928 -0.004709 -0.003825 -0.003924      0.003738   \n",
      "425556 -0.002505 -0.003928 -0.004709 -0.003825 -0.003924      0.002919   \n",
      "425614 -0.002505 -0.003928 -0.004709 -0.003825 -0.003924      0.002741   \n",
      "425730 -0.002505 -0.003825 -0.004709 -0.003825 -0.003825      0.003468   \n",
      "425788 -0.002505 -0.003825 -0.004294 -0.003825 -0.003825      0.003597   \n",
      "418107 -0.002767 -0.004746 -0.005459 -0.002923 -0.003980      0.002703   \n",
      "418165 -0.002767 -0.004746 -0.005459 -0.002923 -0.003980      0.002972   \n",
      "418223 -0.002030 -0.004746 -0.005459 -0.002923 -0.003925      0.003136   \n",
      "\n",
      "        avgreturn_100  avgreturn_180  avgreturn_30  avgreturn_60  momentum_10  \\\n",
      "425846       0.001470       0.001067      0.002497      0.001727     0.025578   \n",
      "425672       0.001415       0.000970      0.002304      0.001748     0.031712   \n",
      "425498       0.001392       0.000913      0.002342      0.001616     0.033642   \n",
      "425556       0.001302       0.000908      0.002098      0.001547     0.026270   \n",
      "425614       0.001340       0.000944      0.002132      0.001523     0.024669   \n",
      "425730       0.001492       0.000982      0.002311      0.001743     0.031211   \n",
      "425788       0.001473       0.001081      0.002313      0.001739     0.032374   \n",
      "418107       0.000668       0.000432      0.002457      0.001473     0.027027   \n",
      "418165       0.000705       0.000452      0.002176      0.001515     0.029723   \n",
      "418223       0.000701       0.000406      0.001900      0.001568     0.031361   \n",
      "\n",
      "        momentum_100  momentum_180  momentum_30  momentum_60   skew_10  \\\n",
      "425846      0.141114      0.184594     0.067423     0.096685  0.089570   \n",
      "425672      0.135861      0.167774     0.062209     0.097872 -0.405963   \n",
      "425498      0.132198      0.157960     0.063229     0.090508 -0.516827   \n",
      "425556      0.123732      0.157059     0.056658     0.086654 -0.149637   \n",
      "425614      0.127271      0.163257     0.057572     0.085299 -0.114373   \n",
      "425730      0.143238      0.169901     0.062393     0.097622 -0.360019   \n",
      "425788      0.141434      0.187048     0.062456     0.097374 -0.420045   \n",
      "418107      0.064804      0.074240     0.071257     0.085429  1.404387   \n",
      "418165      0.068369      0.077760     0.063104     0.087857  1.299390   \n",
      "418223      0.068023      0.069908     0.055106     0.090939  1.434918   \n",
      "\n",
      "        skew_100  skew_180   skew_30   skew_60  volatility_10  volatility_100  \\\n",
      "425846  0.524153 -0.456094  0.162332  0.345367       0.004321        0.003479   \n",
      "425672  0.443144 -0.783715  0.070376  0.331354       0.004466        0.003558   \n",
      "425498  0.428749 -0.811993 -0.035853  0.336195       0.003681        0.003480   \n",
      "425556  0.469883 -0.808031  0.079844  0.355084       0.004222        0.003475   \n",
      "425614  0.443582 -0.820498  0.064070  0.356562       0.004066        0.003499   \n",
      "425730  0.506067 -0.793394  0.065461  0.334879       0.004490        0.003477   \n",
      "425788  0.522119 -0.466483  0.064061  0.337797       0.004373        0.003478   \n",
      "418107  0.858486  0.127317  0.855112  0.899067       0.005252        0.003973   \n",
      "418165  0.830819  0.113237  0.965960  0.868207       0.005167        0.003975   \n",
      "418223  0.832407  0.125668  1.151437  0.904003       0.004996        0.003977   \n",
      "\n",
      "        volatility_180  volatility_30  volatility_60   MACRO_8   MACRO_0  \\\n",
      "425846        0.003871       0.003988       0.003912 -0.017694  0.004861   \n",
      "425672        0.004100       0.004214       0.003906  0.012282 -0.000896   \n",
      "425498        0.004051       0.003913       0.003853  0.032940  0.001572   \n",
      "425556        0.004055       0.004016       0.003895 -0.013351 -0.008961   \n",
      "425614        0.004066       0.004039       0.003867 -0.001266  0.004444   \n",
      "425730        0.004098       0.004213       0.003906  0.068272  0.003605   \n",
      "425788        0.003871       0.004212       0.003908  0.018839  0.014663   \n",
      "418107        0.004143       0.004574       0.004077 -0.023937  0.007548   \n",
      "418165        0.004146       0.004281       0.004077 -0.023977 -0.000478   \n",
      "418223        0.004117       0.004208       0.004024  0.015599 -0.001758   \n",
      "\n",
      "         MACRO_1   MACRO_2   MACRO_3   MACRO_4   MACRO_5   MACRO_6   MACRO_7  \\\n",
      "425846 -0.001551 -0.023520 -0.003667  0.002625  0.006203  0.009418  0.003442   \n",
      "425672  0.004108  0.013532  0.012375  0.003543 -0.006160  0.010745  0.007140   \n",
      "425498  0.003729  0.024350  0.004686  0.027446 -0.012675  0.022009  0.011988   \n",
      "425556 -0.000177  0.008056  0.022876 -0.000431 -0.025676 -0.016270  0.008929   \n",
      "425614 -0.009202 -0.000406 -0.011127 -0.026724  0.013176  0.001231 -0.005958   \n",
      "425730  0.014230  0.009511 -0.008447 -0.004854  0.001377 -0.013064 -0.009977   \n",
      "425788  0.017713  0.005610  0.009418  0.013747 -0.002063  0.012808  0.027316   \n",
      "418107  0.000000  0.004743 -0.000790  0.006140 -0.000604  0.011491 -0.012839   \n",
      "418165  0.012325 -0.021898 -0.001843  0.000000  0.000000 -0.029276 -0.021077   \n",
      "418223  0.033421 -0.006151  0.009492 -0.021360 -0.035067 -0.014709 -0.030115   \n",
      "\n",
      "             lgb       xgb     ridge        NN  \n",
      "425846  0.667003  0.587473  0.496416  0.386843  \n",
      "425672  0.665032  0.590192  0.499086  0.483363  \n",
      "425498  0.662941  0.589137  0.499512  0.516736  \n",
      "425556  0.662941  0.587473  0.498142  0.568853  \n",
      "425614  0.662941  0.587473  0.496964  0.587801  \n",
      "425730  0.660004  0.590192  0.498989  0.511318  \n",
      "425788  0.656876  0.590192  0.498503  0.411917  \n",
      "418107  0.646882  0.593500  0.500558  0.696598  \n",
      "418165  0.646882  0.589966  0.501532  0.596189  \n",
      "418223  0.646882  0.596208  0.504197  0.519175  \n",
      "          VaR_10   VaR_100   VaR_180    VaR_30    VaR_60  avgreturn_10  \\\n",
      "425846 -0.002505 -0.003825 -0.004294 -0.003351 -0.003825      0.002842   \n",
      "425672 -0.002505 -0.003924 -0.004709 -0.003825 -0.003825      0.003524   \n",
      "425498 -0.001599 -0.003928 -0.004709 -0.003825 -0.003924      0.003738   \n",
      "425556 -0.002505 -0.003928 -0.004709 -0.003825 -0.003924      0.002919   \n",
      "425614 -0.002505 -0.003928 -0.004709 -0.003825 -0.003924      0.002741   \n",
      "425730 -0.002505 -0.003825 -0.004709 -0.003825 -0.003825      0.003468   \n",
      "425788 -0.002505 -0.003825 -0.004294 -0.003825 -0.003825      0.003597   \n",
      "418107 -0.002767 -0.004746 -0.005459 -0.002923 -0.003980      0.002703   \n",
      "418165 -0.002767 -0.004746 -0.005459 -0.002923 -0.003980      0.002972   \n",
      "418223 -0.002030 -0.004746 -0.005459 -0.002923 -0.003925      0.003136   \n",
      "\n",
      "        avgreturn_100  avgreturn_180  avgreturn_30  avgreturn_60  momentum_10  \\\n",
      "425846       0.001470       0.001067      0.002497      0.001727     0.025578   \n",
      "425672       0.001415       0.000970      0.002304      0.001748     0.031712   \n",
      "425498       0.001392       0.000913      0.002342      0.001616     0.033642   \n",
      "425556       0.001302       0.000908      0.002098      0.001547     0.026270   \n",
      "425614       0.001340       0.000944      0.002132      0.001523     0.024669   \n",
      "425730       0.001492       0.000982      0.002311      0.001743     0.031211   \n",
      "425788       0.001473       0.001081      0.002313      0.001739     0.032374   \n",
      "418107       0.000668       0.000432      0.002457      0.001473     0.027027   \n",
      "418165       0.000705       0.000452      0.002176      0.001515     0.029723   \n",
      "418223       0.000701       0.000406      0.001900      0.001568     0.031361   \n",
      "\n",
      "        momentum_100  momentum_180  momentum_30  momentum_60   skew_10  \\\n",
      "425846      0.141114      0.184594     0.067423     0.096685  0.089570   \n",
      "425672      0.135861      0.167774     0.062209     0.097872 -0.405963   \n",
      "425498      0.132198      0.157960     0.063229     0.090508 -0.516827   \n",
      "425556      0.123732      0.157059     0.056658     0.086654 -0.149637   \n",
      "425614      0.127271      0.163257     0.057572     0.085299 -0.114373   \n",
      "425730      0.143238      0.169901     0.062393     0.097622 -0.360019   \n",
      "425788      0.141434      0.187048     0.062456     0.097374 -0.420045   \n",
      "418107      0.064804      0.074240     0.071257     0.085429  1.404387   \n",
      "418165      0.068369      0.077760     0.063104     0.087857  1.299390   \n",
      "418223      0.068023      0.069908     0.055106     0.090939  1.434918   \n",
      "\n",
      "        skew_100  skew_180   skew_30   skew_60  volatility_10  volatility_100  \\\n",
      "425846  0.524153 -0.456094  0.162332  0.345367       0.004321        0.003479   \n",
      "425672  0.443144 -0.783715  0.070376  0.331354       0.004466        0.003558   \n",
      "425498  0.428749 -0.811993 -0.035853  0.336195       0.003681        0.003480   \n",
      "425556  0.469883 -0.808031  0.079844  0.355084       0.004222        0.003475   \n",
      "425614  0.443582 -0.820498  0.064070  0.356562       0.004066        0.003499   \n",
      "425730  0.506067 -0.793394  0.065461  0.334879       0.004490        0.003477   \n",
      "425788  0.522119 -0.466483  0.064061  0.337797       0.004373        0.003478   \n",
      "418107  0.858486  0.127317  0.855112  0.899067       0.005252        0.003973   \n",
      "418165  0.830819  0.113237  0.965960  0.868207       0.005167        0.003975   \n",
      "418223  0.832407  0.125668  1.151437  0.904003       0.004996        0.003977   \n",
      "\n",
      "        volatility_180  volatility_30  volatility_60   MACRO_8   MACRO_0  \\\n",
      "425846        0.003871       0.003988       0.003912 -0.017694  0.004861   \n",
      "425672        0.004100       0.004214       0.003906  0.012282 -0.000896   \n",
      "425498        0.004051       0.003913       0.003853  0.032940  0.001572   \n",
      "425556        0.004055       0.004016       0.003895 -0.013351 -0.008961   \n",
      "425614        0.004066       0.004039       0.003867 -0.001266  0.004444   \n",
      "425730        0.004098       0.004213       0.003906  0.068272  0.003605   \n",
      "425788        0.003871       0.004212       0.003908  0.018839  0.014663   \n",
      "418107        0.004143       0.004574       0.004077 -0.023937  0.007548   \n",
      "418165        0.004146       0.004281       0.004077 -0.023977 -0.000478   \n",
      "418223        0.004117       0.004208       0.004024  0.015599 -0.001758   \n",
      "\n",
      "         MACRO_1   MACRO_2   MACRO_3   MACRO_4   MACRO_5   MACRO_6   MACRO_7  \\\n",
      "425846 -0.001551 -0.023520 -0.003667  0.002625  0.006203  0.009418  0.003442   \n",
      "425672  0.004108  0.013532  0.012375  0.003543 -0.006160  0.010745  0.007140   \n",
      "425498  0.003729  0.024350  0.004686  0.027446 -0.012675  0.022009  0.011988   \n",
      "425556 -0.000177  0.008056  0.022876 -0.000431 -0.025676 -0.016270  0.008929   \n",
      "425614 -0.009202 -0.000406 -0.011127 -0.026724  0.013176  0.001231 -0.005958   \n",
      "425730  0.014230  0.009511 -0.008447 -0.004854  0.001377 -0.013064 -0.009977   \n",
      "425788  0.017713  0.005610  0.009418  0.013747 -0.002063  0.012808  0.027316   \n",
      "418107  0.000000  0.004743 -0.000790  0.006140 -0.000604  0.011491 -0.012839   \n",
      "418165  0.012325 -0.021898 -0.001843  0.000000  0.000000 -0.029276 -0.021077   \n",
      "418223  0.033421 -0.006151  0.009492 -0.021360 -0.035067 -0.014709 -0.030115   \n",
      "\n",
      "             lgb       xgb     ridge        NN  \n",
      "425846  0.667003  0.587473  0.496416  0.386843  \n",
      "425672  0.665032  0.590192  0.499086  0.483363  \n",
      "425498  0.662941  0.589137  0.499512  0.516736  \n",
      "425556  0.662941  0.587473  0.498142  0.568853  \n",
      "425614  0.662941  0.587473  0.496964  0.587801  \n",
      "425730  0.660004  0.590192  0.498989  0.511318  \n",
      "425788  0.656876  0.590192  0.498503  0.411917  \n",
      "418107  0.646882  0.593500  0.500558  0.696598  \n",
      "418165  0.646882  0.589966  0.501532  0.596189  \n",
      "418223  0.646882  0.596208  0.504197  0.519175  \n",
      "425846     EQ_1\n",
      "425672     EQ_1\n",
      "425498     EQ_1\n",
      "425556     EQ_1\n",
      "425614     EQ_1\n",
      "425730     EQ_1\n",
      "425788     EQ_1\n",
      "418107    FXD_2\n",
      "418165    FXD_2\n",
      "418223    FXD_2\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n",
      "Top 10 worst assets for LightGBM Classifier:\n",
      "Int64Index([425785, 425959, 425843, 426133, 425901, 426017, 461778, 461604,\n",
      "            373092, 425321],\n",
      "           dtype='int64')\n",
      "425785    EQS_8\n",
      "425959    EQS_8\n",
      "425843    EQS_8\n",
      "426133    EQS_8\n",
      "425901    EQS_8\n",
      "426017    EQS_8\n",
      "461778     FI_9\n",
      "461604     FI_9\n",
      "373092     FI_5\n",
      "425321    EQS_8\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n",
      "Top 10 best assets for XGBoost Classifier:\n",
      "Int64Index([332212, 332270, 332328, 434318, 434260, 434376, 434202, 332386,\n",
      "            361143, 332444],\n",
      "           dtype='int64')\n",
      "332212    FXD_5\n",
      "332270    FXD_5\n",
      "332328    FXD_5\n",
      "434318    EQ_13\n",
      "434260    EQ_13\n",
      "434376    EQ_13\n",
      "434202    EQ_13\n",
      "332386    FXD_5\n",
      "361143     FI_4\n",
      "332444    FXD_5\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n",
      "Top 10 worst assets for XGBoost Classifier:\n",
      "Int64Index([416824, 416534, 416708, 416592, 416766, 417172, 416882, 416940,\n",
      "            416650, 461662],\n",
      "           dtype='int64')\n",
      "416824    FI_5\n",
      "416534    FI_5\n",
      "416708    FI_5\n",
      "416592    FI_5\n",
      "416766    FI_5\n",
      "417172    FI_5\n",
      "416882    FI_5\n",
      "416940    FI_5\n",
      "416650    FI_5\n",
      "461662    FI_9\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n",
      "Top 10 best assets for Ridge Classifier:\n",
      "Int64Index([488762, 460001, 459943, 488704, 490386, 488646, 490444, 458957,\n",
      "            488530, 460175],\n",
      "           dtype='int64')\n",
      "488762    FXE_3\n",
      "460001    EQS_4\n",
      "459943    EQS_4\n",
      "488704    FXE_3\n",
      "490386    FXE_3\n",
      "488646    FXE_3\n",
      "490444    FXE_3\n",
      "458957    EQS_4\n",
      "488530    FXE_3\n",
      "460175    EQS_4\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n",
      "Top 10 worst assets for Ridge Classifier:\n",
      "Int64Index([458203, 458261, 458319, 458087, 458145, 487950, 459569, 459566,\n",
      "            459565, 459578],\n",
      "           dtype='int64')\n",
      "458203    EQS_4\n",
      "458261    EQS_4\n",
      "458319    EQS_4\n",
      "458087    EQS_4\n",
      "458145    EQS_4\n",
      "487950    FXE_3\n",
      "459569     FI_4\n",
      "459566    FI_14\n",
      "459565    FI_13\n",
      "459578    FXD_3\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n",
      "Top 10 best assets for NN Classifier:\n",
      "Int64Index([488762, 488704, 488646, 378794, 490270, 490212, 460178, 490038,\n",
      "            459961, 490328],\n",
      "           dtype='int64')\n",
      "488762    FXE_3\n",
      "488704    FXE_3\n",
      "488646    FXE_3\n",
      "378794    FXE_3\n",
      "490270    FXE_3\n",
      "490212    FXE_3\n",
      "460178    EQS_7\n",
      "490038    FXE_3\n",
      "459961     EQ_5\n",
      "490328    FXE_3\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n",
      "Top 10 worst assets for NN Classifier:\n",
      "Int64Index([466047, 327993, 331520, 328051, 467265, 465989, 467323, 465931,\n",
      "            459588, 460009],\n",
      "           dtype='int64')\n",
      "466047    EQ_16\n",
      "327993    EQS_4\n",
      "331520    FXD_9\n",
      "328051    EQS_4\n",
      "467265    EQ_16\n",
      "465989    EQ_16\n",
      "467323    EQ_16\n",
      "465931    EQ_16\n",
      "459588    FXE_3\n",
      "460009    EQ_10\n",
      "Name: asset, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the assets\n",
    "\n",
    "print(\"Top 10 best assets for LightGBM Classifier:\")\n",
    "print(top_10_best_assets_lgb)\n",
    "print(X_test.loc[top_10_best_assets_lgb.index])\n",
    "print(df.loc[top_10_best_assets_lgb.index]['asset'])\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 worst assets for LightGBM Classifier:\")\n",
    "print(top_10_worst_assets_lgb.index)\n",
    "print(df.loc[top_10_worst_assets_lgb.index]['asset'])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top 10 best assets for XGBoost Classifier:\")\n",
    "print(top_10_best_assets_xgb.index)\n",
    "print(df.loc[top_10_best_assets_xgb.index]['asset'])\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 worst assets for XGBoost Classifier:\")\n",
    "print(top_10_worst_assets_xgb.index)\n",
    "print(df.loc[top_10_worst_assets_xgb.index]['asset'])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top 10 best assets for Ridge Classifier:\")\n",
    "print(top_10_best_assets_ridge.index)\n",
    "print(df.loc[top_10_best_assets_ridge.index]['asset'])\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 worst assets for Ridge Classifier:\")\n",
    "print(top_10_worst_assets_ridge.index)\n",
    "print(df.loc[top_10_worst_assets_ridge.index]['asset'])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Top 10 best assets for NN Classifier:\")\n",
    "print(top_10_best_assets_NN.index)\n",
    "print(df.loc[top_10_best_assets_NN.index]['asset'])\n",
    "print(\"\\n\")\n",
    "print(\"Top 10 worst assets for NN Classifier:\")\n",
    "print(top_10_worst_assets_NN.index)\n",
    "print(df.loc[top_10_worst_assets_NN.index]['asset'])\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

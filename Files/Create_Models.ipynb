{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from func import *\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\master3\\Master-thesis\\Files\\func.py:21: DtypeWarning: Columns (15,27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_read = pd.read_csv(filename, sep=seperator)\n"
     ]
    }
   ],
   "source": [
    "filename = 'all_data_anonymized.csv'\n",
    "date_col = 'todate'\n",
    "start_date = '1980-01-01'\n",
    "seperator = ';'\n",
    "fill = 0\n",
    "lim = 5\n",
    "df_read = load_and_preprocess_data(filename, date_col, start_date, seperator, fill, lim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [5, 10, 20, 40, 60, 100, 180, 240, 360, 480]\n",
    "window_m = [10, 30, 60, 100, 180]\n",
    "assets = df_read.columns\n",
    "df_feat = add_features(df_read, window_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_days = 9\n",
    "\n",
    "return_column_shift = 'avgreturn'\n",
    "volatility_column_shift = 'volatility'\n",
    "df = add_y_col(df_feat, df_read, date_col, target_days, return_column_shift, volatility_column_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, X_test, y_train, y_eval, y_test, X_train_eval, y_train_eval = prepare_training_dataset(df, date_col, shuffle=False, train_split=0.25, eval_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'alpha': 10.0}\n",
      "Best accuracy on evaluation set: 0.5086063469241039\n"
     ]
    }
   ],
   "source": [
    "param_grid_alpha = {'alpha': [0.1, 1.0, 10.0]}\n",
    "ridge_best, grid_search = optimize_and_train_ridge(X_train, y_train, X_train_eval, y_train_eval, param_grid_alpha)\n",
    "\n",
    "# save the model to pickle\n",
    "\n",
    "\n",
    "filename = 'ridge_model.pickle'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(ridge_best, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oscar Azrak\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\sklearn.py:889: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_best, best_params = optimize_and_train_xgb(X_train, y_train, X_eval, y_eval, param_grid_xgb)\n",
    "\n",
    "# save the model to pickle\n",
    "\n",
    "filename = 'xgb_model.pickle'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(xgb_best, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 47869, number of negative: 49501\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 97370, number of used features: 34\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491620 -> initscore=-0.033525\n",
      "[LightGBM] [Info] Start training from score -0.033525\n",
      "Best hyperparameters: {'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 200, 'num_leaves': 31}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 95752, number of negative: 98988\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8670\n",
      "[LightGBM] [Info] Number of data points in the train set: 194740, number of used features: 34\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.491691 -> initscore=-0.033237\n",
      "[LightGBM] [Info] Start training from score -0.033237\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_lgb = {\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [31, 63, 127, 255]\n",
    "}\n",
    "\n",
    "# Call the function with your datasets and hyperparameter grid\n",
    "lgb_best, best_params = optimize_and_train_lgb(X_train, y_train, X_eval, y_eval, param_grid_lgb)\n",
    "\n",
    "# save the model to pickle\n",
    "\n",
    "filename = 'lgb_model.pickle'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(lgb_best, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oscar Azrak\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 715us/step - accuracy: 0.5239 - loss: 0.2493 - val_accuracy: 0.5396 - val_loss: 0.2478\n",
      "Epoch 2/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 670us/step - accuracy: 0.5321 - loss: 0.2484 - val_accuracy: 0.5456 - val_loss: 0.2470\n",
      "Epoch 3/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 676us/step - accuracy: 0.5416 - loss: 0.2475 - val_accuracy: 0.5390 - val_loss: 0.2476\n",
      "Epoch 4/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 662us/step - accuracy: 0.5449 - loss: 0.2468 - val_accuracy: 0.5566 - val_loss: 0.2454\n",
      "Epoch 5/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 673us/step - accuracy: 0.5523 - loss: 0.2460 - val_accuracy: 0.5566 - val_loss: 0.2451\n",
      "Epoch 6/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 668us/step - accuracy: 0.5533 - loss: 0.2456 - val_accuracy: 0.5604 - val_loss: 0.2443\n",
      "Epoch 7/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 672us/step - accuracy: 0.5612 - loss: 0.2442 - val_accuracy: 0.5673 - val_loss: 0.2435\n",
      "Epoch 8/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 676us/step - accuracy: 0.5622 - loss: 0.2438 - val_accuracy: 0.5705 - val_loss: 0.2430\n",
      "Epoch 9/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 667us/step - accuracy: 0.5678 - loss: 0.2427 - val_accuracy: 0.5699 - val_loss: 0.2422\n",
      "Epoch 10/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 660us/step - accuracy: 0.5718 - loss: 0.2418 - val_accuracy: 0.5726 - val_loss: 0.2417\n",
      "Epoch 11/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 660us/step - accuracy: 0.5717 - loss: 0.2417 - val_accuracy: 0.5791 - val_loss: 0.2403\n",
      "Epoch 12/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 661us/step - accuracy: 0.5756 - loss: 0.2408 - val_accuracy: 0.5823 - val_loss: 0.2398\n",
      "Epoch 13/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 662us/step - accuracy: 0.5775 - loss: 0.2404 - val_accuracy: 0.5829 - val_loss: 0.2396\n",
      "Epoch 14/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 677us/step - accuracy: 0.5802 - loss: 0.2397 - val_accuracy: 0.5836 - val_loss: 0.2390\n",
      "Epoch 15/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 677us/step - accuracy: 0.5839 - loss: 0.2387 - val_accuracy: 0.5878 - val_loss: 0.2385\n",
      "Epoch 16/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 661us/step - accuracy: 0.5866 - loss: 0.2380 - val_accuracy: 0.5927 - val_loss: 0.2367\n",
      "Epoch 17/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 676us/step - accuracy: 0.5875 - loss: 0.2378 - val_accuracy: 0.5923 - val_loss: 0.2363\n",
      "Epoch 18/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 664us/step - accuracy: 0.5891 - loss: 0.2372 - val_accuracy: 0.5940 - val_loss: 0.2359\n",
      "Epoch 19/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 664us/step - accuracy: 0.5892 - loss: 0.2370 - val_accuracy: 0.5939 - val_loss: 0.2360\n",
      "Epoch 20/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 661us/step - accuracy: 0.5948 - loss: 0.2358 - val_accuracy: 0.5947 - val_loss: 0.2353\n",
      "Epoch 21/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 662us/step - accuracy: 0.5916 - loss: 0.2357 - val_accuracy: 0.5945 - val_loss: 0.2358\n",
      "Epoch 22/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 669us/step - accuracy: 0.5950 - loss: 0.2352 - val_accuracy: 0.5987 - val_loss: 0.2345\n",
      "Epoch 23/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 677us/step - accuracy: 0.5938 - loss: 0.2352 - val_accuracy: 0.6005 - val_loss: 0.2339\n",
      "Epoch 24/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 678us/step - accuracy: 0.5992 - loss: 0.2338 - val_accuracy: 0.6008 - val_loss: 0.2342\n",
      "Epoch 25/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 690us/step - accuracy: 0.5989 - loss: 0.2338 - val_accuracy: 0.6003 - val_loss: 0.2337\n",
      "Epoch 26/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 674us/step - accuracy: 0.6015 - loss: 0.2335 - val_accuracy: 0.6046 - val_loss: 0.2321\n",
      "Epoch 27/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 670us/step - accuracy: 0.6022 - loss: 0.2329 - val_accuracy: 0.6065 - val_loss: 0.2325\n",
      "Epoch 28/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 693us/step - accuracy: 0.6010 - loss: 0.2328 - val_accuracy: 0.6062 - val_loss: 0.2317\n",
      "Epoch 29/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 724us/step - accuracy: 0.6013 - loss: 0.2328 - val_accuracy: 0.6058 - val_loss: 0.2319\n",
      "Epoch 30/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 767us/step - accuracy: 0.6032 - loss: 0.2323 - val_accuracy: 0.6091 - val_loss: 0.2312\n",
      "Epoch 31/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 724us/step - accuracy: 0.6042 - loss: 0.2322 - val_accuracy: 0.6090 - val_loss: 0.2313\n",
      "Epoch 32/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 765us/step - accuracy: 0.6070 - loss: 0.2311 - val_accuracy: 0.6093 - val_loss: 0.2308\n",
      "Epoch 33/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 710us/step - accuracy: 0.6084 - loss: 0.2308 - val_accuracy: 0.6102 - val_loss: 0.2306\n",
      "Epoch 34/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 731us/step - accuracy: 0.6073 - loss: 0.2309 - val_accuracy: 0.6096 - val_loss: 0.2308\n",
      "Epoch 35/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 706us/step - accuracy: 0.6076 - loss: 0.2311 - val_accuracy: 0.6114 - val_loss: 0.2299\n",
      "Epoch 36/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 713us/step - accuracy: 0.6095 - loss: 0.2300 - val_accuracy: 0.6086 - val_loss: 0.2304\n",
      "Epoch 37/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 692us/step - accuracy: 0.6085 - loss: 0.2303 - val_accuracy: 0.6141 - val_loss: 0.2296\n",
      "Epoch 38/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 717us/step - accuracy: 0.6109 - loss: 0.2302 - val_accuracy: 0.6109 - val_loss: 0.2295\n",
      "Epoch 39/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 696us/step - accuracy: 0.6114 - loss: 0.2295 - val_accuracy: 0.6146 - val_loss: 0.2293\n",
      "Epoch 40/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 690us/step - accuracy: 0.6109 - loss: 0.2295 - val_accuracy: 0.6129 - val_loss: 0.2296\n",
      "Epoch 41/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 688us/step - accuracy: 0.6120 - loss: 0.2292 - val_accuracy: 0.6161 - val_loss: 0.2286\n",
      "Epoch 42/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 677us/step - accuracy: 0.6115 - loss: 0.2295 - val_accuracy: 0.6123 - val_loss: 0.2298\n",
      "Epoch 43/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 674us/step - accuracy: 0.6128 - loss: 0.2291 - val_accuracy: 0.6178 - val_loss: 0.2282\n",
      "Epoch 44/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 675us/step - accuracy: 0.6151 - loss: 0.2285 - val_accuracy: 0.6181 - val_loss: 0.2273\n",
      "Epoch 45/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 682us/step - accuracy: 0.6147 - loss: 0.2281 - val_accuracy: 0.6170 - val_loss: 0.2280\n",
      "Epoch 46/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 669us/step - accuracy: 0.6123 - loss: 0.2287 - val_accuracy: 0.6202 - val_loss: 0.2267\n",
      "Epoch 47/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 686us/step - accuracy: 0.6166 - loss: 0.2281 - val_accuracy: 0.6180 - val_loss: 0.2277\n",
      "Epoch 48/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 669us/step - accuracy: 0.6171 - loss: 0.2279 - val_accuracy: 0.6184 - val_loss: 0.2275\n",
      "Epoch 49/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 686us/step - accuracy: 0.6165 - loss: 0.2283 - val_accuracy: 0.6205 - val_loss: 0.2271\n",
      "Epoch 50/50\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 666us/step - accuracy: 0.6183 - loss: 0.2274 - val_accuracy: 0.6221 - val_loss: 0.2268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "NN_model, history, X_test_scaled = train_and_evaluate_NN(X_train_eval, y_train_eval, X_eval, y_eval, X_test, y_test)\n",
    "\n",
    "# save the model \n",
    "\n",
    "model_filename = 'NN_model.h5'\n",
    "NN_model.save(model_filename)\n",
    "\n",
    "X_test_scaled_filename = 'X_test_scaled.pickle'\n",
    "with open(X_test_scaled_filename, 'wb') as file:\n",
    "    pickle.dump(X_test_scaled, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
